{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install addict\n",
    "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/Tesi/deforestation_mapping-tesi/datadrive/forest\")\n",
    "process_dir = data_dir / \"processed\"\n",
    "log_dir = data_dir / \"demo/logs\"\n",
    "\n",
    "args = Dict({\n",
    "    \"batch_size\": 8,\n",
    "    \"run_name\": \"demo\", \n",
    "    \"epochs\": 20,\n",
    "    \"save_every\": 50,\n",
    "    \"loss_type\": \"dice\",\n",
    "    \"device\": \"cuda:0\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/Colab Notebooks/Tesi/deforestation_mapping-tesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Frame to Combine Model with Optimizer\n",
    "\n",
    "This wraps the model and optimizer objects needed in training, so that each\n",
    "training step can be concisely called with a single method (optimize).\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from forest_mapping.models.metrics import *\n",
    "from forest_mapping.models.reg import *\n",
    "from segmentation_models_pytorch.decoders.fpn.model import *\n",
    "from segmentation_models_pytorch.decoders.unet.model import *\n",
    "from segmentation_models_pytorch.decoders.manet.model import *\n",
    "\n",
    "\n",
    "class Framework2:\n",
    "    \"\"\"\n",
    "    Class to Wrap all the Training Steps\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_fn=None, model_opts=None, optimizer_opts=None,\n",
    "                 reg_opts=None, device=None):\n",
    "        \"\"\"\n",
    "        Set Class Attrributes\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.multi_class = True if model_opts.args.classes > 1 else False\n",
    "        self.num_classes = model_opts.args.classes\n",
    "        if loss_fn is None:\n",
    "            if self.multi_class:\n",
    "                loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            else:\n",
    "                loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.loss_fn = loss_fn.to(self.device)\n",
    "        if model_opts.name in [\"FPN\",\"MAnet\",\"Unet\"]:\n",
    "            model_def = globals()[model_opts.name]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model name\")\n",
    "        print(model_def)\n",
    "\n",
    "        self.model = model_def(**model_opts.args).to(self.device)\n",
    "        optimizer_def = getattr(torch.optim, optimizer_opts.name)\n",
    "        self.optimizer = optimizer_def(self.model.parameters(), **optimizer_opts.args)\n",
    "        self.lrscheduler = ReduceLROnPlateau(self.optimizer, \"min\",\n",
    "                                             verbose=True, patience=10,\n",
    "                                             min_lr=1e-6)\n",
    "        self.reg_opts = reg_opts\n",
    "\n",
    "\n",
    "    def optimize(self, x, y):\n",
    "        \"\"\"\n",
    "        Take a single gradient step\n",
    "\n",
    "        Args:\n",
    "            X: raw training data\n",
    "            y: labels\n",
    "        Return:\n",
    "            optimization\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 1, 2).to(self.device)\n",
    "        y = y.permute(0, 3, 1, 2).to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.calc_loss(y_hat, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return y_hat.permute(0, 2, 3, 1), loss.item()\n",
    "\n",
    "    def val_operations(self, val_loss):\n",
    "        \"\"\"\n",
    "        Update the LR Scheduler\n",
    "        \"\"\"\n",
    "        self.lrscheduler.step(val_loss)\n",
    "\n",
    "    def save(self, out_dir, epoch):\n",
    "        \"\"\"\n",
    "        Save a model checkpoint\n",
    "        \"\"\"\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        model_path = Path(out_dir, f\"model_{epoch}.pt\")\n",
    "        optim_path = Path(out_dir, f\"optim_{epoch}.pt\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        torch.save(self.optimizer.state_dict(), optim_path)\n",
    "\n",
    "    def infer(self, x):\n",
    "        \"\"\" Make a prediction for a given x\n",
    "\n",
    "        Args:\n",
    "            x: input x\n",
    "\n",
    "        Return:\n",
    "            Prediction\n",
    "\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 1, 2).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model(x).permute(0, 2, 3, 1)\n",
    "\n",
    "    def segment(self, y_hat):\n",
    "        \"\"\"Predict a class given logits\n",
    "\n",
    "        Args:\n",
    "            y_hat: logits output\n",
    "\n",
    "        Return:\n",
    "            Probability of class in case of binary classification\n",
    "            or one-hot tensor in case of multi class\"\"\"\n",
    "        if self.multi_class:\n",
    "            y_hat = torch.argmax(y_hat, axis=3)\n",
    "            y_hat = torch.nn.functional.one_hot(y_hat, num_classes=self.num_classes)\n",
    "        else:\n",
    "            y_hat = torch.sigmoid(y_hat)\n",
    "        return y_hat\n",
    "\n",
    "    def act(self, logits):\n",
    "        \"\"\"Applies activation function based on the model\n",
    "        Args:\n",
    "            y_hat: logits output\n",
    "        Returns:\n",
    "            logits after applying activation function\"\"\"\n",
    "\n",
    "        if self.multi_class:\n",
    "            y_hat = torch.nn.Softmax(3)(logits)\n",
    "        else:\n",
    "            y_hat = torch.sigmoid(logits)\n",
    "        return y_hat\n",
    "\n",
    "    def calc_loss(self, y_hat, y):\n",
    "        \"\"\" Compute loss given a prediction\n",
    "\n",
    "        Args:\n",
    "            y_hat: Prediction\n",
    "            y: Label\n",
    "\n",
    "        Return:\n",
    "            Loss values\n",
    "\n",
    "        \"\"\"\n",
    "        y_hat = y_hat.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        if self.multi_class:\n",
    "            y = torch.argmax(y, dim=1)\n",
    "            y = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "        for reg_type in self.reg_opts.keys():\n",
    "            reg_fun = globals()[reg_type]\n",
    "            penalty = reg_fun(\n",
    "                self.model.parameters(),\n",
    "                self.reg_opts[reg_type],\n",
    "                self.device\n",
    "            )\n",
    "            loss += penalty\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def metrics(self, y_hat, y, metrics_opts):\n",
    "        \"\"\" Loop over metrics in train.yaml\n",
    "\n",
    "        Args:\n",
    "            y_hat: Predictions\n",
    "            y: Labels\n",
    "            metrics_opts: Metrics specified in the train.yaml\n",
    "\n",
    "        Return:\n",
    "            results\n",
    "\n",
    "        \"\"\"\n",
    "        y_hat = y_hat.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        results = {}\n",
    "        for k, metric in metrics_opts.items():\n",
    "            if \"threshold\" in metric.keys():\n",
    "                y_hat = y_hat > metric[\"threshold\"]\n",
    "\n",
    "            metric_fun = globals()[k]\n",
    "            results[k] = metric_fun(y_hat, y)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U imagecodecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forest_mapping.data.data import fetch_loaders\n",
    "import forest_mapping.train as tr\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from forest_mapping.models.metrics import diceloss\n",
    "import yaml\n",
    "import torch\n",
    "import json\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import ee\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "img=tifffile.imread('/content/drive/MyDrive/Colab Notebooks/Tesi/deforestation_mapping-tesi/datadrive/forest/processed/Training/images/S2A_MSIL2A_20200111T142701_N0213_R053_T20NQG_20200111T164651_02_11.tif')\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "f, bxarr = plt.subplots(2,2)\n",
    "bxarr[0,0].imshow(img[...,0])\n",
    "bxarr[0,1].imshow(img[...,1])\n",
    "bxarr[1,0].imshow(img[...,2])\n",
    "bxarr[1,1].imshow(img[...,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install earthpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per vedere immagine RGB del dataset sulle foreste\n",
    "import earthpy.plot as ep\n",
    "image = np.dstack((img[...,0],img[...,1],img[...,2]))\n",
    "\n",
    "image2 = image.transpose(2,0,1)\n",
    "\n",
    "ep.plot_rgb(image2,\n",
    "            title=\"RGB Composite Image\",\n",
    "            stretch=True,\n",
    "            str_clip=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"Amazon Dataset.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = [\"No Forest\", \"Forest\"]\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [float(self.CLASSES.index(cls)) for cls in classes]\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = tifffile.imread(self.images_fps[i]).astype(np.float32, order='C')/ 32768.0\n",
    "\n",
    "\n",
    "        # Calcolo ndvi\n",
    "        red = image[...,0]\n",
    "        nir = image[...,3]\n",
    "        ndvi = (nir.astype(float) - red.astype(float)) / (nir + red)\n",
    "\n",
    "        # Calcolo msavi\n",
    "        msavi = nir.astype(float) + 0.5 - (0.5 * np.sqrt((2 * nir.astype(float) + 1)**2 - 8 * (nir.astype(float) - (2 * red.astype(float)))))\n",
    "\n",
    "        image = np.dstack((image, ndvi.astype(np.float32, order='C'), msavi.astype(np.float32, order='C')))\n",
    "\n",
    "\n",
    "        # extract mask\n",
    "        mask_forest = tifffile.imread(self.masks_fps[i])\n",
    "\n",
    "        mask_background = mask_forest.copy()\n",
    "        mask_background[mask_background == 1] = 2\n",
    "        mask_background[mask_background == 0] = 1\n",
    "        mask_background[mask_background == 2] = 0\n",
    "        mask = np.dstack((mask_background,mask_forest)).astype('float')\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Dict(yaml.safe_load(open(\"conf/train_fpn.yaml\", \"r\")))\n",
    "train_folder = \"Training\"\n",
    "x_train_dir = process_dir/train_folder/\"images\"\n",
    "y_train_dir = process_dir/train_folder/\"masks\"\n",
    "dev_folder = \"Validation\"\n",
    "x_valid_dir = process_dir/dev_folder/\"images\"\n",
    "y_valid_dir = process_dir/dev_folder/\"masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"No Forest\", \"Forest\"]\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir,\n",
    "    y_train_dir,\n",
    "    classes = CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir,\n",
    "    y_valid_dir,\n",
    "    classes = CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "loss_fn = None\n",
    "outchannels = conf.model_opts.args.classes\n",
    "if args.loss_type == \"dice\":\n",
    "    loss_fn = diceloss(\n",
    "        act=torch.nn.Softmax(dim=1), \n",
    "        w=[0.65, 0.75],\n",
    "        outchannels=outchannels, \n",
    "        label_smoothing=0.2\n",
    "    )\n",
    "    \n",
    "frame = Framework2(\n",
    "    model_opts=conf.model_opts,\n",
    "    optimizer_opts=conf.optim_opts,\n",
    "    reg_opts=conf.reg_opts,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "enc_name = conf.model_opts.args.encoder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "writer = SummaryWriter(f\"{data_dir}/{args.run_name}/fpn_vi_final_union/{enc_name}/logs/\")\n",
    "writer.add_text(\"Arguments\", json.dumps(vars(args)))\n",
    "writer.add_text(\"Configuration Parameters\", json.dumps(conf))\n",
    "out_dir = f\"{data_dir}/{args.run_name}/models/fpn_vi_final_union/{enc_name}/\"\n",
    "\n",
    "best_epoch, best_iou = None, 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_d = {}\n",
    "    loss_d[\"train\"], metrics_train = tr.train_epoch(train_loader, frame, conf.metrics_opts)\n",
    "    tr.log_metrics(writer, metrics_train, loss_d[\"train\"], epoch, \"train\", mask_names=conf.log_opts.mask_names)\n",
    "    loss_d[\"val\"], metrics_val = tr.validate(valid_loader, frame, conf.metrics_opts)\n",
    "    tr.log_metrics(writer, metrics_val, loss_d[\"val\"], epoch, \"val\", mask_names=conf.log_opts.mask_names)\n",
    "\n",
    "    # save model\n",
    "    writer.add_scalars(\"Loss\", loss_d, epoch)\n",
    "    if (epoch + 1) % args.save_every == 0:\n",
    "        frame.save(out_dir, epoch)\n",
    "        tr.log_images(writer, frame, next(iter(train_loader)), epoch)\n",
    "        tr.log_images(writer, frame, next(iter(valid_loader)), epoch, \"val\")\n",
    "\n",
    "    if best_iou <= metrics_train['IoU'][0]:\n",
    "        best_iou  = metrics_train['IoU'][0]\n",
    "        best_epoch = epoch\n",
    "        frame.save(out_dir, \"best\")\n",
    "\n",
    "    print(f\"{epoch}/{args.epochs} | train loss: {loss_d['train']} | val loss: {loss_d['val']} | val_recall: {metrics_val['recall'][0]}\")\n",
    "\n",
    "frame.save(out_dir, \"final\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forest_mapping.data.data import fetch_loaders\n",
    "import forest_mapping.train as tr\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from forest_mapping.models.metrics import diceloss\n",
    "import yaml\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Dict(yaml.safe_load(open(\"conf/train_fpn.yaml\", \"r\")))\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model_dir = f\"{data_dir}/demo/models/fpn_vi_final/densenet161/model_best.pt\"\n",
    "outchannels = conf.model_opts.args.classes\n",
    "\n",
    "if outchannels > 1:\n",
    "    loss_weight = [1 for _ in range(outchannels)]\n",
    "    loss_weight[-1] = 0 # background\n",
    "    loss_fn = diceloss(act=torch.nn.Softmax(dim=1), w=loss_weight,\n",
    "                               outchannels=outchannels)\n",
    "else:\n",
    "    loss_fn = diceloss()\n",
    "\n",
    "frame = Framework2(\n",
    "    model_opts=conf.model_opts,\n",
    "    optimizer_opts=conf.optim_opts,\n",
    "    reg_opts=conf.reg_opts,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")\n",
    "    \n",
    "net = frame.model\n",
    "net.load_state_dict(torch.load(model_dir))#, map_location=torch.device('cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "slices_dir = f\"{process_dir}/Test/images/S2*\"\n",
    "pred_dir = f\"{process_dir}/predictions_png/fpn_vi_final/\"\n",
    "pred_dir2 = f\"{process_dir}/predictions_numpy/fpn_vi_final/\"\n",
    "    \n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "if not os.path.exists(pred_dir2):\n",
    "    os.makedirs(pred_dir2)\n",
    "\n",
    "slices = glob.glob(slices_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_inference_time = 0\n",
    "for s in slices:\n",
    "    filename = s.split(\"/\")[-1].replace(\"tif\",\"png\")\n",
    "    filename2 = s.split(\"/\")[-1].replace(\"tif\", \"npy\")\n",
    "    \n",
    "\n",
    "    img = tifffile.imread(s).astype(np.float32)\n",
    "    # Calcolo ndvi\n",
    "    red = img[...,0]      \n",
    "    nir = img[...,3]\n",
    "    ndvi = (nir.astype(float) - red.astype(float)) / (nir + red)\n",
    "\n",
    "    # Calcolo msavi\n",
    "    msavi = nir.astype(float) + 0.5 - (0.5 * np.sqrt((2 * nir.astype(float) + 1)**2 - 8 * (nir.astype(float) - (2 * red.astype(float)))))\n",
    "\n",
    "    image = np.dstack((img, ndvi.astype(np.float32, order='C'), msavi.astype(np.float32, order='C')))\n",
    "\n",
    "    \n",
    "    inp_tensor=torch.from_numpy(np.expand_dims(image,axis=0)).permute((0,3,1,2)).to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    output = net(inp_tensor)\n",
    "    output_np = output.detach().cpu().numpy()\n",
    "    output_np = np.transpose(output_np[0], (1,2,0))\n",
    "    output_np = np.argmax(output_np, axis=2)\n",
    "    total_inference_time += (time.time() - start)\n",
    "    plt.imsave(f\"{pred_dir}{filename}\", output_np, vmin=0, vmax=3)\n",
    "    np.save(f\"{pred_dir2}{filename2}\", output_np)\n",
    "print(f\"Total inference time: {total_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii as ba\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir datadrive/forest/demo/unet/densenet161/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score, accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support, f1_score, jaccard_score, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import rasterio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images and use the metrics\n",
    "y_true = list()\n",
    "y_pred_tmp = list()\n",
    "y_pred_tmp1 = list()\n",
    "y_pred_tmp2 = list()\n",
    "\n",
    "pred_png = list()\n",
    "\n",
    "\n",
    "test_path = \"/content/drive/MyDrive/Colab Notebooks/Tesi/deforestation_mapping-tesi/datadrive/forest/processed\"\n",
    "true = 'Test/masks'\n",
    "pred = 'predictions_numpy/manet'\n",
    "\n",
    "names_images = sorted(os.listdir(os.path.join(test_path,true)))\n",
    "pred_png = sorted(os.listdir(os.path.join(test_path,pred)))\n",
    "\n",
    "for image in names_images:\n",
    "    path = os.path.join(test_path,true,image)\n",
    "\n",
    "    mask_forest = tifffile.imread(path)\n",
    "    mask_background = mask_forest.copy()   \n",
    "    mask_background[mask_background == 1] = 2\n",
    "    mask_background[mask_background == 0] = 1\n",
    "    mask_background[mask_background == 2] = 0\n",
    "    mask = np.dstack((mask_background,mask_forest)).astype('float')\n",
    "\n",
    "    y_true.append(mask)\n",
    "\n",
    "for prediction in pred_png:\n",
    "\n",
    "    path = os.path.join(test_path,pred,prediction)\n",
    "    image = np.load(path)\n",
    "    y_pred_tmp.append(image)\n",
    "\n",
    "y_true = np.asarray(y_true)\n",
    "y_pred = np.asarray(y_pred_tmp)\n",
    "\n",
    "y_true_tmp = np.zeros_like(y_pred)\n",
    "y_true_tmp[y_true[:,:,:,0]==1] = 0\n",
    "y_true_tmp[y_true[:,:,:,1]==1] = 1\n",
    "\n",
    "y_true = y_true_tmp\n",
    "print(y_true.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "\n",
    "y_true = y_true.ravel()\n",
    "y_pred = y_pred.ravel()\n",
    "\n",
    "f1_score_teste = f1_score(y_true, y_pred,average='macro')\n",
    "print(\"F1-score: \",f1_score_teste)\n",
    "\n",
    "accuracy_score_teste = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy: \",accuracy_score_teste)\n",
    "\n",
    "all_metrics = precision_recall_fscore_support(y_true, y_pred,average=\"macro\")\n",
    "print(\"Precision, recall and fscore: \", all_metrics)\n",
    "\n",
    "iou_score_test = jaccard_score(y_true, y_pred, average=None)\n",
    "print(\"IoU: \",iou_score_test)\n",
    "\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion matrix: \")\n",
    "print(cf)\n",
    "\n",
    "\n",
    "fbeta = fbeta_score(y_true, y_pred, average='macro', beta=0.01)\n",
    "print(\"Fbeta: \", fbeta)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following matrix shows classification percentages\n",
    "\n",
    "mat = list()\n",
    "mat.append(cf[0]/2611101)\n",
    "mat.append(cf[1]/2631779)\n",
    "mat = np.array(mat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=mat, display_labels=['No Forest', 'Forest'])\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
